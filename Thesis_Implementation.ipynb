{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################Thesis_Implementation################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#later add the libraries with links and what each library does just as an information \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "\n",
    "import re \n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mythril Disassembly code used to transpate bytecodes to opcodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.about the disassembler \n",
    "#using the mythril python library for disassembly \n",
    "#https://github.com/ConsenSys/mythril\n",
    "#takes bytecode and ouputs machine level opcodes \n",
    "#write basic working of the disassembler code later\n",
    "\n",
    "from mythril.ether import asm,util\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "\n",
    "class Disassembly:\n",
    "\n",
    "    def __init__(self, code):\n",
    "        self.instruction_list = asm.disassemble(util.safe_decode(code))\n",
    "        self.xrefs = []\n",
    "        self.func_to_addr = {}\n",
    "        self.addr_to_func = {}\n",
    "\n",
    "        try:\n",
    "            mythril_dir = os.environ['MYTHRIL_DIR']\n",
    "        except KeyError:\n",
    "            mythril_dir = os.path.join(os.path.expanduser('~'), \".mythril\")\n",
    "\n",
    "        # Load function signatures\n",
    "\n",
    "        signatures_file = os.path.join(mythril_dir, 'signatures.json')\n",
    "\n",
    "        if not os.path.exists(signatures_file):\n",
    "            logging.info(\"Missing function signature file. Resolving of function names disabled.\")\n",
    "            signatures = {}\n",
    "        else:\n",
    "            with open(signatures_file) as f:\n",
    "                signatures = json.load(f)\n",
    "\n",
    "        # Parse jump table & resolve function names\n",
    "\n",
    "        jmptable_indices = asm.find_opcode_sequence([\"PUSH4\", \"EQ\"], self.instruction_list)\n",
    "\n",
    "        for i in jmptable_indices:\n",
    "            func_hash = self.instruction_list[i]['argument']\n",
    "            try:\n",
    "                func_name = signatures[func_hash]\n",
    "            except KeyError:\n",
    "                func_name = \"_function_\" + func_hash\n",
    "\n",
    "            try:\n",
    "                offset = self.instruction_list[i+2]['argument']\n",
    "                jump_target = int(offset, 16)\n",
    "\n",
    "                self.func_to_addr[func_name] = jump_target\n",
    "                self.addr_to_func[jump_target] = func_name\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "    def get_easm(self):\n",
    "\n",
    "        return asm.instruction_list_to_easm(self.instruction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVM opcodes as provides in the Ethereum Yellow Paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.EVM opcodes \n",
    "#source Ethereum yellow paper \n",
    "#Document what each opcode is doing \n",
    "EVM_OPCODE_SET = {'STOP', 'ADD', 'MUL', 'SUB', 'DIV', 'SDIV', 'MOD', 'SMOD', 'ADDMOD', 'MULMOD', 'EXP', 'SIGNEXTEND',\n",
    "                 'LT','GT', 'SLT', 'SGT', 'EQ', 'ISZERO', 'AND', 'OR', 'XOR', 'NOT', 'BYTE','SHA3',\n",
    "                 'ADDRESS', 'BALANCE', 'ORIGIN', 'CALLER', 'CALLVALUE', 'CALLDATALOAD', 'CALLDATASIZE', 'CALLDATACOPY', \n",
    "                  'CODESIZE', 'CODECOPY', 'GASPRICE','EXTCODESIZE','EXTCODECOPY', 'RETURNDATASIZE', 'RETURNDATACOPY',\n",
    "                 'BLOCKHASH', 'COINBASE', 'TIMESTAMP', 'NUMBER', 'DIFFICULTY', 'GASLIMIT','POP','MLOAD', 'MSTORE', 'MSTORE8'\n",
    "                 'SLOAD', 'SSTORE', 'JUMP', 'JUMPI', 'PC', 'MSIZE', 'GAS', 'JUMPDEST','PUSH1','PUSH2','PUSH3','PUSH4',\n",
    "                  'PUSH5','PUSH6','PUSH7','PUSH8','PUSH9','PUSH10','PUSH11','PUSH12','PUSH13','PUSH14','PUSH15','PUSH16',\n",
    "                 'PUSH17','PUSH18','PUSH19','PUSH20','PUSH21','PUSH22','PUSH23','PUSH24','PUSH25','PUSH26','PUSH27',\n",
    "                 'PUSH28','PUSH29','PUSH30','PUSH31','PUSH32','DUP1','DUP2','DUP3','DUP4','DUP5','DUP6','DUP7','DUP8','DUP9',\n",
    "                 'DUP10','DUP11','DUP12','DUP13','DUP14','DUP15','DUP16','DUP17','DUP18','SWAP1','SWAP2','SWAP3','SWAP4',\n",
    "                 'SWAP5','SWAP6','SWAP7','SWAP8','SWAP9','SWAP10','SWAP11','SWAP12','SWAP13','SWAP14','SWAP15','SWAP16',\n",
    "                 'LOG0','LOG1','LOG2','LOG3','LOG4','CREATE', 'CAL', 'CALLCODE','RETURN','DELEGATECALL' ,'STATICCALL','REVERT', \n",
    "                  'INVALID','SELFDESTRUCT'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data, Translating bytecodes to opcodes, Processing opcodes and get rid of arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "#df = pd.read_csv('/Users/ravishaggarwal/Desktop/contracts.csv',sep = ',')\n",
    "df = pd.read_csv(\"/Users/ravishaggarwal/Desktop/thesis_impl/implementations/data/toy_data/new_data1.csv\",sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bytecodes that are to be translated to opcodes \n",
    "bytecode = df['result.code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the list of opcodes translated from bytecodes \n",
    "opcode = (list(map(lambda x: Disassembly(x).get_easm(),bytecode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting opcodes list to dataframe\n",
    "opcode_df = pd.DataFrame(opcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming the opcodes dataframe\n",
    "opcode_df.columns = ['opcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the opcodes data frame into the original data frame \n",
    "df['opcode'] = opcode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the bytecode and the translated opcodes \n",
    "new_df = df[df.columns[3:]]\n",
    "#new_df = df.columns['result.code','opcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df is the new dataframe consist of translated opcodes \n",
    "#new_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################CSV_WRITE############################################################\n",
    "#writing the result into the csv file for future usage\n",
    "#new_df.to_csv('new_data.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text from the punctuations \n",
    "def cleanpunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#|{|}]',r'',str(sentence))\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r'',cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining clean opcodes \n",
    "#set of opcodes without arguments are output as a CSV file with the name clean_ops.csv\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "with open('/Users/ravishaggarwal/Desktop/new_data.csv') as input_file, open('/Users/ravishaggarwal/Desktop/clean_ops.csv', 'w') as output_file:\n",
    "    r = csv.reader(input_file)\n",
    "    row = next(r)\n",
    "    counter = 0\n",
    "    str1 = ''\n",
    "    i = 0\n",
    "    final_string = []\n",
    "    for row in r:\n",
    "        var1 = row[1].split()\n",
    "        #print(var1)\n",
    "        #counter = counter + 1\n",
    "        #print(counter)\n",
    "        #print(var1)\n",
    "        a = set(var1)&EVM_OPCODE_SET\n",
    "        #b = list(a)\n",
    "        \n",
    "        #print(b)\n",
    "        #print(a,file = output_file)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "        \n",
    "                \n",
    "            \n",
    "            #print(sent)\n",
    "            \n",
    "        c = cleanpunc(a)\n",
    "        print(''.join(c),file = output_file)\n",
    "        \n",
    "        #print(a)\n",
    "        #print(b,file = output_file)\n",
    "        #output_file.write(a)\n",
    "        #counter = counter + 1\n",
    "        #print(counter)\n",
    "        \n",
    "            \n",
    "        #for i in var1:\n",
    "            #print(i)\n",
    "            #if(i in EVM_OPCODE_SET):\n",
    "                #output_file.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking rows in csv \n",
    "#with open('/Users/ravishaggarwal/Desktop/new_data.csv') as file:\n",
    "#    r = csv.reader(file)\n",
    "#    row_count = sum(1 for row in r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the Bigram Sparse matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams matrix\n",
    "#bigram sparse matrix is output as a csv file with the name bigrams.csv\n",
    "import csv\n",
    "import nltk\n",
    "from collections import Counter\n",
    "fo = open(\"/Users/ravishaggarwal/Desktop/clean_ops.csv\")\n",
    "fo1 = fo.readlines()\n",
    "counter_sum = Counter()\n",
    "\n",
    "for line in fo1:\n",
    "    \n",
    "    \n",
    "    #tokens = nltk.word_tokenize(line)\n",
    "    \n",
    "    \n",
    "    bigrams = list(nltk.bigrams(line.split()))  #list of all the possible bigrams \n",
    "    #print(bigrams)\n",
    "    #print(len(bigrams))\n",
    "    #fdist = nltk.FreqDist(bigrams)\n",
    "    #for k,v in fdist.items():\n",
    "        #print (k,v)\n",
    "    \n",
    "    \n",
    "    bigramsC = Counter(bigrams)\n",
    "    \n",
    "    \n",
    "    #counting the no of bigrams combination in each contract \n",
    "    #print(bigramsC)\n",
    "    \n",
    "    \n",
    "    #tokensC = Counter(tokens)\n",
    "    #both_counters = bigramsC + tokensC\n",
    "    counter_sum = bigramsC\n",
    "    #print(counter_sum)\n",
    "    #print(counter_sum)\n",
    "    #labels, values = zip(*counter_sum.items())\n",
    "    #print(values)\n",
    "\n",
    "    #print(tokensC)\n",
    "    \n",
    "   \n",
    "    \n",
    "with open('/Users/ravishaggarwal/Desktop/#bigrams.csv', 'w', newline='') as csvfile:\n",
    "    header = sorted(counter_sum, key=lambda x: str(type(x)))\n",
    "    #print(header)\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for line in fo1:\n",
    "          #tokens = nltk.word_tokenize(line)\n",
    "          bigrams = list(nltk.bigrams(line.split()))\n",
    "          bigramsC = Counter(bigrams)\n",
    "          \n",
    "          #tokensC = Counter(tokens)\n",
    "          #both_counters = bigramsC + tokensC\n",
    "          cs = dict(counter_sum)\n",
    "          bc = dict(bigramsC)\n",
    "          row = {}\n",
    "          for element in list(cs):\n",
    "                if element in list(bc):\n",
    "                  row[element] = bc[element]\n",
    "                else:\n",
    "                  row[element] = 0\n",
    "          writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#here the idea is to read the bigrams.csv and so column vise sum of all the header as header is our bigram unique vocab\n",
    "import csv\n",
    "from __future__ import print_function\n",
    "\n",
    "sep = ','\n",
    "with open('/Users/ravishaggarwal/Desktop/bigrams.csv', 'rt') as f:\n",
    "    #columns = f.readline().strip().split(sep)\n",
    "    reader = csv.reader(f)\n",
    "    col = next(reader)\n",
    "    #print(len(columns))\n",
    "    rows = [0] * len(col)\n",
    "    for line in f.readlines():\n",
    "        data = line.strip().split(sep)\n",
    "        for i, cell in enumerate(data, start=0):\n",
    "            rows[i] += float(cell)\n",
    "\n",
    "print(col)\n",
    "print(rows)\n",
    "print(len(col))\n",
    "print(len(rows))\n",
    "\n",
    "labels_bigram = col \n",
    "value_bigram  = rows\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking out the dimension of bigrams.csv matrix csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"/Users/ravishaggarwal/Desktop/bigrams.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    fieldnames = reader.fieldnames\n",
    "    row_count = sum(1 for row in reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the top three rows of the bigrams.csv \n",
    "import csv\n",
    "afile = open(\"/Users/ravishaggarwal/Desktop/bigrams.csv\", 'r+')\n",
    "csvReader1 = csv.reader(afile)\n",
    "for i in range(3):\n",
    "    print (csvReader1.__next__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Consider the opcodes (Bag Of Words) interpretations \n",
    "### Calculating the frequency distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "opcodes = ['CALLDATACOPY', 'PUSH1', 'GAS', 'PUSH20', 'PUSH2', 'CALLVALUE', 'SUB', 'DUP1', 'CALLCODE', 'RETURN', 'CALLDATASIZE']\n",
    "no of opcodes = 11 \n",
    "bigrams = [('CALLDATACOPY', 'PUSH1'), ('PUSH1', 'GAS'), ('GAS', 'PUSH20'), ('PUSH20', 'PUSH2'), ('PUSH2', 'CALLVALUE'), ('CALLVALUE', 'SUB'), ('SUB', 'DUP1'), ('DUP1', 'CALLCODE'), ('CALLCODE', 'RETURN'), ('RETURN', 'CALLDATASIZE')]\n",
    "no of bigrams = 10 \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/ravishaggarwal/Desktop/c_ops.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import inaugural\n",
    "# extract the datataset in raw format, you can also extract it in other formats as well\n",
    "text = str(data)\n",
    "wordcloud = WordCloud(max_font_size=20).generate(text)\n",
    "plt.figure(figsize=(16,12))\n",
    "# plot wordcloud in matplotlib\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open(\"/Users/ravishaggarwal/Desktop/c_ops.txt\") as f:\n",
    "    wordcount = Counter(f.read().split())\n",
    "    #print(wordcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, values = zip(*wordcount.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of 136 opcodes in the EVM_OPCODES_SET 131 are repeated frequently in the data \n",
    "len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort your values in descending order\n",
    "indSort = np.argsort(values)[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange your data\n",
    "labels1 = np.array(labels)[indSort]\n",
    "values1 = np.array(values)[indSort]\n",
    "indexes1 = np.arange(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = labels1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = values1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram top 20 values \n",
    "#labels, values = zip(*wordcount.items())\n",
    "\n",
    "#idx = np.arange(len(labels1))\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.bar(l, v,width=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bottom 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = labels1[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = values1[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram bottom 20 values \n",
    "#labels, values = zip(*wordcount.items())\n",
    "\n",
    "#idx = np.arange(len(labels1))\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.bar(l1, v1,width=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distribution \n",
    "with open(\"/Users/ravishaggarwal/Desktop/c_ops.txt\") as f:\n",
    "    words = [word for word in f.read().split()]\n",
    "    freqdist = nltk.FreqDist(words)\n",
    "    plt.figure(figsize=(30,10))\n",
    "    freqdist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the opcodes (Bigram interpretations\n",
    "### Calculating the frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text from the punctuations \n",
    "def cleanpunc1(sentence):\n",
    "    cleaned = re.sub(r'[?|!||[|]]',r'',str(sentence))\n",
    "    cleaned = re.sub(r'[.|||[|]|]',r'',cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geting bigrams output as bi_gram.csv\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "with open('/Users/ravishaggarwal/Desktop/clean_ops.csv') as input_file, open('/Users/ravishaggarwal/Desktop/bi_gram1.csv', 'w') as output_file:\n",
    "    r = csv.reader(input_file)\n",
    "    row = next(r)\n",
    "    counter = 0\n",
    "    str1 = ''\n",
    "    i = 0\n",
    "    final_string = []\n",
    "    for row in r:\n",
    "        var1 = row[0].split()\n",
    "        \n",
    "        #print(var1)\n",
    "        bigrams = list(nltk.bigrams(var1))\n",
    "        \n",
    "        #print(bigrams)\n",
    "        \n",
    "        \n",
    "        #print(var1)\n",
    "        #counter = counter + 1\n",
    "        #print(counter)\n",
    "        #print(var1)\n",
    "        #a = set(var1)&EVM_OPCODE_SET\n",
    "        #b = list(a)\n",
    "        \n",
    "        #print(b)\n",
    "        #print(a,file = output_file)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "        \n",
    "                \n",
    "            \n",
    "            #print(sent)\n",
    "            \n",
    "        c = cleanpunc1(bigrams)\n",
    "        #print(c.rstrip('\\n'))\n",
    "        print(c,file = output_file)\n",
    "        #print(c.rstrip('\\n'),file = output_file)\n",
    "        #print(a)\n",
    "        #print(b,file = output_file)\n",
    "        #output_file.write(a)\n",
    "        #counter = counter + 1\n",
    "        #print(counter)\n",
    "        \n",
    "            \n",
    "        #for i in var1:\n",
    "            #print(i)\n",
    "            #if(i in EVM_OPCODE_SET):\n",
    "                #output_file.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here the idea is to read the bigrams.csv and so column vise sum of all the header as header is our bigram unique vocab\n",
    "import csv\n",
    "from __future__ import print_function\n",
    "\n",
    "sep = ','\n",
    "with open('/Users/ravishaggarwal/Desktop/bigrams.csv', 'rt') as f:\n",
    "    #columns = f.readline().strip().split(sep)\n",
    "    reader = csv.reader(f)\n",
    "    col = next(reader)\n",
    "    #print(len(columns))\n",
    "    rows = [0] * len(col)\n",
    "    for line in f.readlines():\n",
    "        data = line.strip().split(sep)\n",
    "        for i, cell in enumerate(data, start=0):\n",
    "            rows[i] += float(cell)\n",
    "\n",
    "print(col)\n",
    "print(rows)\n",
    "print(len(col))\n",
    "print(len(rows))\n",
    "\n",
    "labels_bigram = col \n",
    "value_bigram  = rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value_bigram \n",
    "# sort your values in descending order\n",
    "indSort = np.argsort(value_bigram)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_bigram\n",
    "# rearrange your data\n",
    "labels_bigram1 = np.array(labels_bigram)[indSort]\n",
    "values_bigram1 = np.array(value_bigram)[indSort]\n",
    "indexes_bigram1 = np.arange(len(labels_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_bigram = labels_bigram1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_bigram = values_bigram1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#labels, values = zip(*wordcount.items())\n",
    "\n",
    "#idx = np.arange(len(labels1))\n",
    "plt.figure(figsize=(57,57))\n",
    "plt.bar(l_bigram, v_bigram,width=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_bigram1 = labels_bigram1[-500:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_bigram1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_bigram1 = values_bigram1[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_bigram1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#labels, values = zip(*wordcount.items())\n",
    "\n",
    "#idx = np.arange(len(labels1))\n",
    "plt.figure(figsize=(57,57))\n",
    "plt.bar(l_bigram1, v_bigram1,width=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels, values = zip(*wordcount.items())\n",
    "\n",
    "#idx = np.arange(len(labels1))\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.bar(labels_bigram, value_bigram,width=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contract Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open('/Users/ravishaggarwal/Desktop/bi_gram.txt') as f:\n",
    "    c = Counter(f)\n",
    "    dups = [t for t in c.most_common() if t[1] > 1]\n",
    "    dups_dict = {row: count for row, count in c.most_common() if count > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, value = zip(*dups_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value[-8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels, values = zip(*wordcount.items())\n",
    "\n",
    "#idx = np.arange(len(labels1))\n",
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "b = [1017390,870791,579340,376008,344381,212472,158860,122783,89134,78287]\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.bar(a,b,width= .2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking and printing\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "dataset = '/Users/ravishaggarwal/Desktop/bigrams.csv'\n",
    "chunksize_ = 10000\n",
    "dimensions = 1457\n",
    "\n",
    "reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)\n",
    "\n",
    "\n",
    "sklearn_pca = IncrementalPCA(n_components=dimensions)\n",
    "\n",
    "for chunk in reader:\n",
    "    #print('chunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunk')\n",
    "    #print(chunk)\n",
    "    #y = chunk.pop(\"Y\")\n",
    "    #print(chunk)\n",
    "    #print('shape of chunk',chunk)\n",
    "    sklearn_pca.partial_fit(chunk)\n",
    "    print('chunking and printing')\n",
    "    Xchunk = sklearn_pca.transform(chunk)\n",
    "    \n",
    "    db = DBSCAN(eps=0.05, min_samples=100).fit_predict(Xchunk[:200])\n",
    "    \n",
    "    print(db)\n",
    "    break\n",
    "    '''\n",
    "    #print(Xchunk)\n",
    "    #reduced_dim = Xchunk[]\n",
    "    #print('shape of  reduced_dim',reduced_dim.shape)\n",
    "    #print(.shape)\n",
    "    a = sklearn_pca.explained_variance_\n",
    "    \n",
    "    print(a)\n",
    "    #print('pca variance explained',a)\n",
    "    print('shape of pca variance explained',a.shape)\n",
    "    percentage_var_explained = sklearn_pca.explained_variance_ / np.sum(sklearn_pca.explained_variance_)\n",
    "    print('shape of percentage_var_explained',percentage_var_explained.shape)\n",
    "    print('percentage variance explained',percentage_var_explained)\n",
    "    cum_var_explained = np.cumsum(percentage_var_explained)\n",
    "    print('cum_var_explained',cum_var_explained)\n",
    "    print('shape of cum_var_explained',cum_var_explained.shape)\n",
    "    # Plot the PCA spectrum\n",
    "    plt.figure(1, figsize=(10, 14))\n",
    "    plt.clf()\n",
    "    plt.plot(cum_var_explained, linewidth=2)\n",
    "    plt.axis('tight')\n",
    "    plt.axis([0,250,0,1])\n",
    "    plt.grid()\n",
    "    plt.xlabel('n_components')\n",
    "    plt.ylabel('Cumulative_explained_variance')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Xtransformed = np.vstack((Xchunk))\n",
    "    #with open('/Users/ravishaggarwal/Desktop/pca.csv','w') as outfile:\n",
    "    #    print((Xtransformed ),file = outfile)\n",
    "        \n",
    "    \n",
    "\n",
    "#Computed mean per feature\n",
    "#mean = sklearn_pca.mean_\n",
    "#print(mean)\n",
    "# and stddev\n",
    "#stddev = np.sqrt(sklearn_pca.var_)\n",
    "#print(stddev)\n",
    "\n",
    "Xtransformed = None\n",
    "for chunk in pd.read_csv(dataset, sep = ',', chunksize = chunksize_):\n",
    "    #y = chunk.pop(\"Y\")\n",
    "    Xchunk = sklearn_pca.transform(chunk)\n",
    "    #print(Xchunk)\n",
    "    if (Xtransformed is None):\n",
    "        Xtransformed = Xchunk\n",
    "    else:\n",
    "        Xtransformed = np.vstack((Xtransformed, Xchunk))\n",
    "        print(Xtransformed)\n",
    "     \n",
    "        #np.savetxt('test.txt', Xtransformed, delimiter=',')\n",
    "        \n",
    "        #with open('/Users/ravishaggarwal/Desktop/pca.csv','w') as outfile:\n",
    "            #print((Xtransformed),file = outfile)\n",
    "    #else:\n",
    "'''       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '/Users/ravishaggarwal/Desktop/bigrams.csv'\n",
    "\n",
    "db = DBSCAN(eps=0.05, min_samples=100).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import IncrementalPCA\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "\n",
    "dataset = '/Users/ravishaggarwal/Desktop/bigrams.csv'\n",
    "chunksize_ = 10000\n",
    "dimensions = 1456\n",
    "\n",
    "reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)\n",
    "\n",
    "# Create a TSVD\n",
    "tsvd = TruncatedSVD(n_components= dimensions)\n",
    "#sklearn_pca = IncrementalPCA(n_components=dimensions)\n",
    "\n",
    "for chunk in reader:\n",
    "    #print('chunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunk')\n",
    "    #print(chunk)\n",
    "    #y = chunk.pop(\"Y\")\n",
    "    print('chunking and printing')\n",
    "    Xchunk = tsvd.fit(chunk).transform(chunk)\n",
    "    #sklearn_pca.partial_fit(chunk)\n",
    "    \n",
    "    #Xchunk = sklearn_pca.transform(chunk)\n",
    "    print(Xchunk)\n",
    "    percentage_var_explained = tsvd.explained_variance_ / np.sum(tsvd.explained_variance_)\n",
    "    \n",
    "    cum_var_explained = np.cumsum(percentage_var_explained)\n",
    "    # Plot the PCA spectrum\n",
    "    plt.figure(1, figsize=(6, 4))\n",
    "    plt.clf()\n",
    "    plt.plot(cum_var_explained, linewidth=2)\n",
    "    plt.axis('tight')\n",
    "    plt.grid()\n",
    "    plt.xlabel('n_components')\n",
    "    plt.ylabel('Cumulative_explained_variance')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Xtransformed = np.vstack((Xchunk))\n",
    "    #with open('/Users/ravishaggarwal/Desktop/pca.csv','w') as outfile:\n",
    "    #    print((Xtra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(10000, 200)\n",
      "chunking and printing\n",
      "(5929, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "with open('/Users/ravishaggarwal/Desktop/bigrams.csv','rt') as input_file, open('/Users/ravishaggarwal/Desktop/pca_transform_data.csv', 'w') as output_file:\n",
    "    dataset = input_file\n",
    "    chunksize_ = 10000\n",
    "    dimensions = 1457\n",
    "    reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)\n",
    "    sklearn_pca = IncrementalPCA(n_components=dimensions)\n",
    "    for chunk in reader:\n",
    "        sklearn_pca.partial_fit(chunk)\n",
    "        print('chunking and printing')\n",
    "        Xchunk = sklearn_pca.transform(chunk)\n",
    "        a = Xchunk[:,:200]\n",
    "        #pprint(a)\n",
    "        b = np.asarray(a)\n",
    "        #np.savetxt('/Users/ravishaggarwal/Desktop/pca_transform_data1.csv', a, delimiter=\",\")\n",
    "        print(b.shape)\n",
    "        csvWriter = csv.writer(output_file,delimiter=',')\n",
    "        csvWriter.writerows(b)\n",
    "        #for row in a:\n",
    "            #b = float(row)\n",
    "        #    print(row.shape)\n",
    "        #    csvWriter.writerows(b)\n",
    "        #print(*a,file = output_file)\n",
    "        #for row in a:\n",
    "            #for elem in row:\n",
    "                #print(elem.shape)\n",
    "                #print(elem, end=' ')#,file = output_file)\n",
    "\n",
    "        #print(Xchunk.shape)\n",
    "        #for s in Xchunk:\n",
    "            #a = s[:100]\n",
    "            #print(*a.shape)\n",
    "            #print(*a,file = output_file)#,file = output_file)\n",
    "        #print(Xchunk,file = output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dimred = dd.read_csv('/Users/ravishaggarwal/Desktop/pca_transform_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4675928"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dimred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dimred = pd.read_csv('/Users/ravishaggarwal/Desktop/pca_transform_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4675928, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_dimred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow method \n",
    "#finding elbow method in amazon revie is difficult \n",
    "#X = df\n",
    "\n",
    "#distortions = []\n",
    "\n",
    "#K = range(1,5)\n",
    "#for k in K:\n",
    "#    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "#    kmeanModel.fit(X)\n",
    "    #print(kmeanModel.inertia_)\n",
    "#    distortions.append(kmeanModel.inertia_)\n",
    "\n",
    "    \n",
    "    \n",
    "# Plot the elbow\n",
    "#plt.plot(K, distortions, 'bx-')\n",
    "#plt.xlabel('k')\n",
    "#plt.ylabel('Distortion')\n",
    "#plt.title('The Elbow Method showing the optimal k')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dimred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.05, min_samples=100).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = DBSCAN(eps=0.05, min_samples=100).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3b9cf7609d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tolerance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;31m# If the distances are precomputed every job will create a matrix of shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_tolerance\u001b[0;34m(X, tol)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mvariances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_variance_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mvariances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3146\u001b[0;31m                          **kwargs)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Note that if dtype is not of inexact type then arraymean will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# not be either.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0marrmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         arrmean = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "x = df\n",
    "#num_clusters = 2\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter = 100,n_init = 1)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
